# Docker Compose Configuration for Mental Health Platform
# Production-ready orchestration with HIPAA compliance

version: '3.9'

services:
  # Frontend Application
  frontend:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        NODE_ENV: production
        VITE_API_BASE_URL: ${API_BASE_URL:-https://api.mentalhealth.app}
        VITE_WS_URL: ${WS_URL:-wss://ws.mentalhealth.app}
        VITE_SENTRY_DSN: ${SENTRY_DSN}
    image: mentalhealth/frontend:${VERSION:-latest}
    container_name: mh-frontend
    restart: unless-stopped
    ports:
      - "80:8080"
      - "443:8443"
    volumes:
      - ./ssl:/etc/nginx/ssl:ro
      - nginx-cache:/var/cache/nginx
      - nginx-logs:/var/log/nginx
    environment:
      - NODE_ENV=production
      - TZ=UTC
    networks:
      - frontend-network
      - monitoring
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
        monitor: 60s
        max_failure_ratio: 0.3
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.frontend.rule=Host(`mentalhealth.app`)"
      - "traefik.http.routers.frontend.tls=true"
      - "traefik.http.routers.frontend.tls.certresolver=letsencrypt"

  # API Backend Service 1
  backend-1:
    image: mentalhealth/backend:${VERSION:-latest}
    container_name: mh-backend-1
    restart: unless-stopped
    environment:
      - NODE_ENV=production
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=redis://redis:6379
      - JWT_SECRET=${JWT_SECRET}
      - ENCRYPTION_KEY=${ENCRYPTION_KEY}
      - SENTRY_DSN=${SENTRY_DSN}
      - PORT=3000
    networks:
      - backend-network
      - database-network
      - cache-network
      - monitoring
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 5s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 1024M
        reservations:
          cpus: '1.0'
          memory: 512M

  # API Backend Service 2 (Load Balancing)
  backend-2:
    image: mentalhealth/backend:${VERSION:-latest}
    container_name: mh-backend-2
    restart: unless-stopped
    environment:
      - NODE_ENV=production
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=redis://redis:6379
      - JWT_SECRET=${JWT_SECRET}
      - ENCRYPTION_KEY=${ENCRYPTION_KEY}
      - SENTRY_DSN=${SENTRY_DSN}
      - PORT=3000
    networks:
      - backend-network
      - database-network
      - cache-network
      - monitoring
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 5s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 1024M
        reservations:
          cpus: '1.0'
          memory: 512M

  # WebSocket Server 1
  ws-1:
    image: mentalhealth/websocket:${VERSION:-latest}
    container_name: mh-websocket-1
    restart: unless-stopped
    environment:
      - NODE_ENV=production
      - REDIS_URL=redis://redis:6379
      - JWT_SECRET=${JWT_SECRET}
      - PORT=3002
    networks:
      - backend-network
      - cache-network
      - monitoring
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3002/health"]
      interval: 30s
      timeout: 5s
      retries: 3

  # WebSocket Server 2
  ws-2:
    image: mentalhealth/websocket:${VERSION:-latest}
    container_name: mh-websocket-2
    restart: unless-stopped
    environment:
      - NODE_ENV=production
      - REDIS_URL=redis://redis:6379
      - JWT_SECRET=${JWT_SECRET}
      - PORT=3002
    networks:
      - backend-network
      - cache-network
      - monitoring
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3002/health"]
      interval: 30s
      timeout: 5s
      retries: 3

  # PostgreSQL Database (Primary)
  postgres:
    image: postgres:15-alpine
    container_name: mh-postgres
    restart: unless-stopped
    environment:
      - POSTGRES_DB=${DB_NAME:-mentalhealth}
      - POSTGRES_USER=${DB_USER:-mhadmin}
      - POSTGRES_PASSWORD=${DB_PASSWORD}
      - POSTGRES_INITDB_ARGS=--encoding=UTF8 --data-checksums
      - POSTGRES_HOST_AUTH_METHOD=scram-sha-256
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./scripts/db-init.sql:/docker-entrypoint-initdb.d/init.sql:ro
      - postgres-backup:/backup
    networks:
      - database-network
      - monitoring
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-mhadmin} -d ${DB_NAME:-mentalhealth}"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2048M
        reservations:
          cpus: '1.0'
          memory: 1024M
    command: >
      postgres
      -c max_connections=200
      -c shared_buffers=256MB
      -c effective_cache_size=1GB
      -c maintenance_work_mem=64MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
      -c work_mem=1310kB
      -c min_wal_size=1GB
      -c max_wal_size=4GB
      -c max_worker_processes=4
      -c max_parallel_workers_per_gather=2
      -c max_parallel_workers=4
      -c max_parallel_maintenance_workers=2
      -c ssl=on
      -c ssl_cert_file=/var/lib/postgresql/server.crt
      -c ssl_key_file=/var/lib/postgresql/server.key

  # Redis Cache
  redis:
    image: redis:7-alpine
    container_name: mh-redis
    restart: unless-stopped
    command: >
      redis-server
      --appendonly yes
      --appendfsync everysec
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis-data:/data
    networks:
      - cache-network
      - monitoring
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M

  # Traefik Reverse Proxy & Load Balancer
  traefik:
    image: traefik:v3.0
    container_name: mh-traefik
    restart: unless-stopped
    command:
      - "--api.dashboard=true"
      - "--api.insecure=false"
      - "--providers.docker=true"
      - "--providers.docker.exposedbydefault=false"
      - "--entrypoints.web.address=:80"
      - "--entrypoints.websecure.address=:443"
      - "--certificatesresolvers.letsencrypt.acme.tlschallenge=true"
      - "--certificatesresolvers.letsencrypt.acme.email=${ACME_EMAIL}"
      - "--certificatesresolvers.letsencrypt.acme.storage=/letsencrypt/acme.json"
      - "--metrics.prometheus=true"
      - "--metrics.prometheus.buckets=0.1,0.3,1.2,5.0"
      - "--accesslog=true"
      - "--log.level=INFO"
    ports:
      - "80:80"
      - "443:443"
      - "8080:8080"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - traefik-certificates:/letsencrypt
      - traefik-logs:/var/log/traefik
    networks:
      - frontend-network
      - backend-network
      - monitoring
    environment:
      - CF_API_EMAIL=${CLOUDFLARE_EMAIL}
      - CF_API_KEY=${CLOUDFLARE_API_KEY}
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.dashboard.rule=Host(`traefik.mentalhealth.app`)"
      - "traefik.http.routers.dashboard.service=api@internal"
      - "traefik.http.routers.dashboard.middlewares=auth"
      - "traefik.http.middlewares.auth.basicauth.users=${TRAEFIK_AUTH}"

  # Prometheus Monitoring
  prometheus:
    image: prom/prometheus:latest
    container_name: mh-prometheus
    restart: unless-stopped
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    networks:
      - monitoring
    ports:
      - "9090:9090"
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1024M
        reservations:
          cpus: '0.5'
          memory: 512M

  # Grafana Dashboard
  grafana:
    image: grafana/grafana:latest
    container_name: mh-grafana
    restart: unless-stopped
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
      - GF_INSTALL_PLUGINS=grafana-piechart-panel
      - GF_SERVER_ROOT_URL=https://grafana.mentalhealth.app
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    networks:
      - monitoring
    ports:
      - "3001:3000"
    depends_on:
      - prometheus
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M

  # Loki for Log Aggregation
  loki:
    image: grafana/loki:latest
    container_name: mh-loki
    restart: unless-stopped
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - ./monitoring/loki-config.yaml:/etc/loki/local-config.yaml:ro
      - loki-data:/loki
    networks:
      - monitoring
    ports:
      - "3100:3100"
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M

  # Promtail for Log Collection
  promtail:
    image: grafana/promtail:latest
    container_name: mh-promtail
    restart: unless-stopped
    command: -config.file=/etc/promtail/config.yml
    volumes:
      - ./monitoring/promtail-config.yaml:/etc/promtail/config.yml:ro
      - /var/log:/var/log:ro
      - nginx-logs:/var/log/nginx:ro
    networks:
      - monitoring
    depends_on:
      - loki

  # Backup Service
  backup:
    image: mentalhealth/backup:${VERSION:-latest}
    container_name: mh-backup
    restart: unless-stopped
    environment:
      - BACKUP_SCHEDULE=${BACKUP_SCHEDULE:-0 2 * * *}
      - POSTGRES_CONNECTION=${DATABASE_URL}
      - S3_BUCKET=${BACKUP_S3_BUCKET}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - ENCRYPTION_KEY=${BACKUP_ENCRYPTION_KEY}
    volumes:
      - postgres-backup:/backup/postgres
      - redis-backup:/backup/redis
    networks:
      - database-network
      - monitoring
    depends_on:
      - postgres
      - redis

networks:
  frontend-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/24
  
  backend-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.21.0.0/24
  
  database-network:
    driver: bridge
    internal: true
    ipam:
      config:
        - subnet: 172.22.0.0/24
  
  cache-network:
    driver: bridge
    internal: true
    ipam:
      config:
        - subnet: 172.23.0.0/24
  
  monitoring:
    driver: bridge
    ipam:
      config:
        - subnet: 172.24.0.0/24

volumes:
  postgres-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH:-./data}/postgres
  
  postgres-backup:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${BACKUP_PATH:-./backups}/postgres
  
  redis-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH:-./data}/redis
  
  redis-backup:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${BACKUP_PATH:-./backups}/redis
  
  nginx-cache:
    driver: local
  
  nginx-logs:
    driver: local
  
  traefik-certificates:
    driver: local
  
  traefik-logs:
    driver: local
  
  prometheus-data:
    driver: local
  
  grafana-data:
    driver: local
  
  loki-data:
    driver: local